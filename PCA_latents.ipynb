{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from XAI import *\n",
    "import pickle\n",
    "from Utilities import set_global_seed\n",
    "import random\n",
    "import torch\n",
    "from tdc.benchmark_group import admet_group\n",
    "from Utilities import *\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA, PLSRegression, PLSCanonical, PLSSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "data_root_atomic = '../your_data_root/'\n",
    "data_root_gap = '../your_data_root/'\n",
    "\n",
    "with open(f'{data_root_atomic}/structures.pkl', 'rb') as f:\n",
    "    mols = pickle.load(f)\n",
    "\n",
    "with open(f'{data_root_atomic}/charges.pkl', 'rb') as f:\n",
    "    charges = pickle.load(f)\n",
    "\n",
    "with open(f'{data_root_atomic}/nmr.pkl', 'rb') as f:\n",
    "    nmrs = pickle.load(f)\n",
    "    \n",
    "with open(f'{data_root_atomic}/fukui_n.pkl', 'rb') as f:\n",
    "    fkn = pickle.load(f)\n",
    "    \n",
    "with open(f'{data_root_atomic}/fukui_e.pkl', 'rb') as f:\n",
    "    fke = pickle.load(f)\n",
    "    \n",
    "with open(f'{data_root_gap}/structures.pkl', 'rb') as f:\n",
    "    mols_gap = pickle.load(f)\n",
    "\n",
    "with open(f'{data_root_gap}/hlgaps.pkl', 'rb') as f:\n",
    "    gaps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "group = admet_group(path = '../data_tdc/')\n",
    "names = group.dataset_names\n",
    "subsample_index = random.sample(range(0, len(mols)), 5000)\n",
    "subsample_index_gap = random.sample(range(0, len(mols_gap)), 5000)\n",
    "mols = [mols[i] for i in subsample_index]\n",
    "mols_gap = [mols_gap[i] for i in subsample_index_gap]\n",
    "gaps = torch.tensor([gaps[i] for i in subsample_index_gap])\n",
    "charges = [torch.tensor(charges[i]) for i in subsample_index]\n",
    "nmrs = [torch.tensor(nmrs[i]) for i in subsample_index]\n",
    "fkn = [torch.tensor(fkn[i]) for i in subsample_index]\n",
    "fke = [torch.tensor(fke[i]) for i in subsample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ccd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "proprietas = {\n",
    "              'charges': charges, \n",
    "              'nmr': nmrs, \n",
    "              'fukui_e': fke, \n",
    "              'fukui_n': fkn,\n",
    "              'homo-lumo': gaps\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'scratch_20L_wide_def_2e-5_16p', \n",
    "    'homo-lumo_20L_wide_def_2e-5_16p', \n",
    "    'masking_20L_wide_def_2e-5_16p', \n",
    "    'charges_20L_wide_def_2e-5_16p', \n",
    "    'nmr_20L_wide_def_2e-5_16p', \n",
    "    'fukui_n_20L_wide_def_2e-5_16p', \n",
    "    'fukui_e_20L_wide_def_2e-5_16p',  \n",
    "    'qm_all_20L_wide_def_2e-5_16p'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a5164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ccas = {}\n",
    "\n",
    "for name in names:\n",
    "    ccas[name] = {}\n",
    "    \n",
    "    for k in tqdm(range(len(models))):\n",
    "        ccas[name][models[k][:-22]] = {}\n",
    "        \n",
    "        for chiave in proprietas.keys():\n",
    "                \n",
    "                if chiave == 'homo-lumo':\n",
    "                    proprieta = proprietas[chiave]\n",
    "\n",
    "                    batch_size = 1\n",
    "                    data = TensorDataset(MoleculeDataset(mols_gap, unpack=True), gaps)\n",
    "                    dlt = DataLoader(data, collate_fn = chained_collate(collate_molecules, torch.stack), shuffle=False, batch_size=batch_size, num_workers=16)\n",
    "\n",
    "                    for model in [models[k]]:\n",
    "\n",
    "                        ckpts = os.listdir(f'./TDC_checkpoints/{model}/{name}_1/')\n",
    "                        ckpt = [c for c in ckpts if c.startswith('epoch')][0]\n",
    "\n",
    "                        checkpoint_path = f'./TDC_checkpoints/{model}/{name}_1/{ckpt}'\n",
    "\n",
    "                        loaded_path_hyper_dict = torch.load(checkpoint_path)['hyper_parameters']\n",
    "\n",
    "                        model = GT(\n",
    "                            checkpoint_path = checkpoint_path,\n",
    "                            **loaded_path_hyper_dict\n",
    "                        )\n",
    "\n",
    "                        model, w = transfer_weights(checkpoint_path, model, device = 'cuda')\n",
    "\n",
    "                        model.eval()\n",
    "                        model.freeze()\n",
    "\n",
    "                        for n, param in model.named_parameters():\n",
    "                            param.requires_grad = False\n",
    "\n",
    "                        tokens = ()\n",
    "                        latents = ()\n",
    "                        properties = ()\n",
    "\n",
    "                        for batch in dlt:\n",
    "                            mol, label = batch\n",
    "                            result, a_, x, xo, L = get_nth_layer(model, batch, device = 'cuda', x_ = True)\n",
    "                            tokens += (mol.atoms,)\n",
    "                            latents += (x,)\n",
    "                            properties += (label,)\n",
    "                    \n",
    "                    props = torch.cat(properties, dim = 0).cpu().numpy()\n",
    "                    lats = torch.cat(latents, dim = 1).cpu().numpy()\n",
    "                    toks = torch.cat(tokens, dim = 1).cpu().numpy()\n",
    "                    toks = toks[0]\n",
    "                    \n",
    "                    scaler_x = StandardScaler()\n",
    "                    scaler_y = StandardScaler()\n",
    "                    X_scaled = scaler_x.fit_transform(lats[0][toks==1])\n",
    "                    y_scaled = scaler_y.fit_transform(props.reshape(-1, 1))\n",
    "                    \n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.5, random_state=42)\n",
    "\n",
    "                    ridge = ElasticNet(alpha=.1, l1_ratio=0.5)\n",
    "                    ridge.fit(X_train, y_train)\n",
    "\n",
    "                    y_pred = ridge.predict(X_test)\n",
    "\n",
    "                    canonical_correlation = r2_score(y_test.reshape(-1), y_pred.reshape(-1))\n",
    "                    print(f'{name} {models[k][:-22]} {chiave} CC: {canonical_correlation}')\n",
    "                    \n",
    "                    ccas[name][models[k][:-22]][chiave] = canonical_correlation\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    proprieta = proprietas[chiave]\n",
    "\n",
    "                    batch_size = 1\n",
    "                    data = TensorDataset(MoleculeDataset(mols, unpack=True), SizedList(proprieta))\n",
    "                    dlt = DataLoader(data, collate_fn = chained_collate(collate_molecules, torch.cat), shuffle=False, batch_size=batch_size, num_workers=16)\n",
    "\n",
    "                    for model in [models[k]]:\n",
    "\n",
    "                        ckpts = os.listdir(f'./TDC_checkpoints/{model}/{name}_1/'\n",
    "                        ckpt = [c for c in ckpts if c.startswith('epoch')][0]\n",
    "\n",
    "                        checkpoint_path = f'./TDC_checkpoints/{model}/{name}_1/{ckpt}'\n",
    "                        \n",
    "                        loaded_path_hyper_dict = torch.load(checkpoint_path)['hyper_parameters']\n",
    "\n",
    "                        model = GT(\n",
    "                            checkpoint_path = checkpoint_path,\n",
    "                            **loaded_path_hyper_dict\n",
    "                        )\n",
    "\n",
    "                        model, w = transfer_weights(checkpoint_path, model, device = 'cuda')\n",
    "\n",
    "                        model.eval()\n",
    "                        model.freeze()\n",
    "\n",
    "                        for n, param in model.named_parameters():\n",
    "                            param.requires_grad = False\n",
    "\n",
    "                        tokens = ()\n",
    "                        latents = ()\n",
    "                        properties = ()\n",
    "\n",
    "                        for batch in dlt:\n",
    "                            mol, label = batch\n",
    "                            result, a_, x, xo, L = get_nth_layer(model, batch, n = 20, device = 'cuda', x_ = True)\n",
    "                            tokens += (mol.atoms,)\n",
    "                            latents += (x,)\n",
    "                            properties += (label,)\n",
    "\n",
    "                    props = torch.cat(properties, dim = 0).cpu().numpy()\n",
    "                    lats = torch.cat(latents, dim = 1).cpu().numpy()\n",
    "                    toks = torch.cat(tokens, dim = 1).cpu().numpy()\n",
    "                    toks = toks[0]\n",
    "\n",
    "                    scaler_x = StandardScaler()\n",
    "                    scaler_y = StandardScaler()\n",
    "                    X_scaled = scaler_x.fit_transform(lats[0][toks!=1])\n",
    "                    toks = toks[toks!=1]\n",
    "                    y_scaled = scaler_y.fit_transform(props.reshape(-1, 1))\n",
    "                    \n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.5, random_state=42)\n",
    "\n",
    "                    ridge = ElasticNet(alpha=.1, l1_ratio=0.5)\n",
    "                    ridge.fit(X_train, y_train)\n",
    "\n",
    "                    y_pred = ridge.predict(X_test)\n",
    "\n",
    "                    canonical_correlation = r2_score(y_test.reshape(-1), y_pred.reshape(-1))\n",
    "                    \n",
    "                    print(f'{name} {models[k][:-22]} {chiave} CC: {canonical_correlation}')\n",
    "                    ccas[name][models[k][:-22]][chiave] = canonical_correlation     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ecba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chiavi_modelli = list(ccas['caco2_wang'].keys())\n",
    "chiavi_proprieta = list(ccas['caco2_wang']['scratch'].keys())\n",
    "\n",
    "tbplotted = {}\n",
    "\n",
    "for m in chiavi_modelli:\n",
    "    \n",
    "    tbplotted[m] = {}\n",
    "    \n",
    "    for p in chiavi_proprieta:\n",
    "        \n",
    "        tmp = []\n",
    "        \n",
    "        for n in names:\n",
    "            \n",
    "            tmp.append(ccas[n][m][p])\n",
    "        \n",
    "        tbplotted[m][p] = [np.mean(tmp), np.std(tmp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbplotted['all'] = tbplotted['qm_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x):\n",
    "    if check == 'qm_all':\n",
    "        return 'all'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "data = tbplotted\n",
    "means = pd.DataFrame({check(key1): {check(key2): values[0] for key2, values in subdict.items()} for key1, subdict in data.items()})\n",
    "stds = pd.DataFrame({check(key1): {check(key2): values[1] for key2, values in subdict.items()} for key1, subdict in data.items()})\n",
    "\n",
    "means = means.round(2)\n",
    "stds = stds.round(2)\n",
    "\n",
    "annotations = means.astype(str) + \" ± \" + stds.astype(str)\n",
    "\n",
    "x_order = chiavi_proprieta + ['all'] + ['scratch']\n",
    "y_order = chiavi_proprieta\n",
    "\n",
    "means = means.reindex(index=y_order, columns=x_order)\n",
    "stds = stds.reindex(index=y_order, columns=x_order)\n",
    "annotations = annotations.reindex(index=y_order, columns=x_order)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(means, annot=annotations, fmt=\"\", linewidths=.5, cmap=\"magma\")\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "plt.yticks(rotation = 45)\n",
    "\n",
    "plt.xlabel('Models', fontsize = 16)\n",
    "plt.ylabel('Pretraining property', fontsize = 16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./permanence_notitle.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca71761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "proprieta = proprietas['nmr']\n",
    "\n",
    "batch_size = 1\n",
    "data = TensorDataset(MoleculeDataset(mols, unpack=True), SizedList(proprieta))\n",
    "dlt = DataLoader(data, collate_fn = chained_collate(collate_molecules, torch.cat), shuffle=False, batch_size=batch_size, num_workers=16)\n",
    "\n",
    "model = models[0]\n",
    "name = names[0]\n",
    "\n",
    "ckpts = os.listdir(f'./TDC_checkpoints/{model}/{name}_1/')\n",
    "ckpt = [c for c in ckpts if c.startswith('epoch')][0]\n",
    "\n",
    "checkpoint_path = f'./TDC_checkpoints/{model}/{name}_1/{ckpt}'\n",
    "\n",
    "loaded_path_hyper_dict = torch.load(checkpoint_path)['hyper_parameters']\n",
    "\n",
    "model = GT(\n",
    "    checkpoint_path = checkpoint_path,\n",
    "    **loaded_path_hyper_dict\n",
    ")\n",
    "\n",
    "model, w = transfer_weights(checkpoint_path, model, device = 'cuda')\n",
    "\n",
    "model.eval()\n",
    "model.freeze()\n",
    "\n",
    "for n, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tokens = ()\n",
    "latents = ()\n",
    "properties = ()\n",
    "\n",
    "for batch in tqdm(dlt):\n",
    "    mol, label = batch\n",
    "    result, a_, x, xo, L = get_nth_layer(model, batch, n = 20, device = 'cuda', x_ = True)\n",
    "    tokens += (mol.atoms,)\n",
    "    latents += (x,)\n",
    "    properties += (label,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeebc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = torch.cat(properties, dim = 0).cpu().numpy()\n",
    "lats = torch.cat(latents, dim = 1).cpu().numpy()\n",
    "toks = torch.cat(tokens, dim = 1).cpu().numpy()\n",
    "toks = toks[0]\n",
    "\n",
    "np.random.seed(42)\n",
    "data = lats[0][toks!=1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_standardized = data\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(data_standardized)\n",
    "\n",
    "toks = toks[toks!=1]\n",
    "color = props\n",
    "\n",
    "colorz = np.tanh((color - np.mean(color))/np.std(color))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(principal_components[:, 0], principal_components[:, 1], c=colorz, edgecolor='k', s=50)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Example Data')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f'Explained variance by component: {explained_variance}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
