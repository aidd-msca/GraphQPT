{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.benchmark_group import admet_group\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "\n",
    "group = admet_group(path = 'data/')\n",
    "names = []\n",
    "\n",
    "for bench in group:\n",
    "    names.append(bench['name'])\n",
    "\n",
    "models = [ \n",
    "         'scratch_20L_wide_def_2e-5_16p', \n",
    "         'qm_all_20L_wide_def_2e-5_16p', \n",
    "         'charges_20L_wide_def_2e-5_16p', \n",
    "         'nmr_20L_wide_def_2e-5_16p', \n",
    "         'fukui_n_20L_wide_def_2e-5_16p', \n",
    "         'fukui_e_20L_wide_def_2e-5_16p',\n",
    "         'masking_20L_wide_def_2e-5_16p',\n",
    "         'homo-lumo_20L_wide_def_2e-5_16p',\n",
    "        ]\n",
    "\n",
    "group = admet_group(path = 'data/')\n",
    "\n",
    "results_collection = {}\n",
    "metric_dict = {}\n",
    "\n",
    "for name in names:\n",
    "    results_collection[name] = {}\n",
    "    \n",
    "    for model in models:\n",
    "        results_collection[name][model] = []\n",
    "        \n",
    "        for seed in [1, 2, 3, 4, 5]:\n",
    "    \n",
    "            benchmark = group.get(name) \n",
    "\n",
    "            predictions = {}\n",
    "\n",
    "            with open(f'./TDC_checkpoints/{model}/{name}_{seed}/predictions_2.pkl', 'rb') as f:\n",
    "                t = pk.load(f)\n",
    "\n",
    "            predictions[name] = np.array(t)\n",
    "            results = group.evaluate(predictions)\n",
    "            results_collection[name][model].append(results[name][list(results[name].keys())[0]])\n",
    "            metric_dict[name] = list(results[name].keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb346a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_results = {}\n",
    "for name in names:\n",
    "    mean_results[name] = {}\n",
    "    for model in models:\n",
    "        temp = []\n",
    "        for seed in [1,2,3,4,5]:\n",
    "            temp.append(results_collection[name][model][seed-1])\n",
    "        mean = np.mean(temp)\n",
    "        mean_results[name][model] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "group = admet_group(path = 'data/')\n",
    "names = []\n",
    "\n",
    "for bench in group:\n",
    "    names.append(bench['name'])\n",
    "    \n",
    "ttest_matrices = {}\n",
    "ordered_models = {}\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    metric = metric_dict[name]\n",
    "    \n",
    "    t_test_matrix = []\n",
    "    \n",
    "    sorting_idx = np.argsort([mean_results[name][k] for k in models])\n",
    "    \n",
    "    if metric!='mae':\n",
    "        sorting_idx = np.flip(sorting_idx)\n",
    "    \n",
    "    ordered_models[name] = np.array(models)[sorting_idx]\n",
    "    \n",
    "    for model in np.array(models)[sorting_idx]:\n",
    "        \n",
    "        tmp = []\n",
    "        \n",
    "        for model_ in np.array(models)[sorting_idx]:\n",
    "            if model!=model_:\n",
    "                tmp.append(ttest_rel(results_collection[name][model], results_collection[name][model_], alternative = 'greater').pvalue)\n",
    "            else:\n",
    "                tmp.append(np.mean(results_collection[name][model]))\n",
    "        \n",
    "        t_test_matrix.append(tmp)\n",
    "        \n",
    "    ttest_matrices[name] = t_test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_significant(matrix):\n",
    "    matrix = np.array(matrix)\n",
    "    mask = matrix>=0.05\n",
    "    matrix[mask]= 0\n",
    "    matrix[~mask] = 1\n",
    "    return matrix\n",
    "\n",
    "name = names[10]\n",
    "print(name)\n",
    "plt.imshow(check_significant(ttest_matrices[name]))\n",
    "plt.yticks(np.arange(0,len(ordered_models[name])), ordered_models[name])\n",
    "plt.xticks(np.arange(0,len(ordered_models[name])), ordered_models[name], rotation = 90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
